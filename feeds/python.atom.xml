<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Breaking Bytes - Python</title><link href="https://breakingbytes.github.io/" rel="alternate"></link><link href="https://breakingbytes.github.io/feeds/python.atom.xml" rel="self"></link><id>https://breakingbytes.github.io/</id><updated>2019-10-07T10:33:00-07:00</updated><subtitle>My personal blog about &lt;a rel="me" href="https://climatejustice.rocks/@mikofski"&gt;me&lt;/a&gt;</subtitle><entry><title>Python Concurrency Primer</title><link href="https://breakingbytes.github.io/python-concurrency-primer.html" rel="alternate"></link><published>2019-10-07T10:33:00-07:00</published><updated>2019-10-07T10:33:00-07:00</updated><author><name>Mark Mikofski</name></author><id>tag:breakingbytes.github.io,2019-10-07:/python-concurrency-primer.html</id><summary type="html">&lt;p&gt;Asynchronous Python workflows for IO and CPU processes on a single machine versus a cluster.&lt;/p&gt;</summary><content type="html">&lt;h1 id="python-threading-versus-multiprocessing"&gt;Python Threading versus Multiprocessing&lt;/h1&gt;
&lt;p&gt;Threading works really well with IO, but not CPU bound processes, hence why there's an &lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;asyncio lib&lt;/a&gt; for IO and a &lt;a href="https://docs.python.org/3/library/multiprocessing.html"&gt;MP lib&lt;/a&gt;. Before Py3, you could only manually create threads using the &lt;a href="https://docs.python.org/3/library/threading.html"&gt;threading lib&lt;/a&gt; and some people who don't like asyncio still prefer to use the manual threading lib, an external lib like &lt;a href="https://trio.readthedocs.io/en/stable/"&gt;trio&lt;/a&gt;, or older packages like &lt;a href="http://www.gevent.org/"&gt;gevents&lt;/a&gt;, etc.&lt;/p&gt;
&lt;p&gt;For an example of using threading for an IO bound process see the &lt;a href="https://github.com/pvlib/pvlib-python/blob/master/pvlib/iotools/ecmwf_macc.py#L168"&gt;&lt;code&gt;pvlib.iotools.get_ecmwf_macc()&lt;/code&gt; method&lt;/a&gt; which returns the thread that calls the ECMWF api client because it is soooooo slow!&lt;/p&gt;
&lt;h1 id="concurrency-in-the-cloud"&gt;Concurrency in the Cloud&lt;/h1&gt;
&lt;p&gt;Curve ball (⚾) - threading and MP work great for a single computer, but when you are on a cluster, such as web applications, then you have more options!  &lt;/p&gt;
&lt;p&gt;In a web app, you usually have a load balancer in front of several workers who answer get requests.&lt;/p&gt;
&lt;p&gt;Therefore you can deligate the work for both IO and CPU bound processes differently than by using either threading, MP, asyncio, trio, gevent, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.celeryproject.org/"&gt;Celery&lt;/a&gt; is probably the most popular task queue for distributed resources especially for web applications.&lt;/p&gt;
&lt;p&gt;I would guess there are task managers other than Celery, but would have to Google around to find a curated list - and many examples recommend Celery, eg: &lt;a href="https://devcenter.heroku.com/articles/celery-heroku"&gt;Heroku&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="inter-process-communication"&gt;Inter-Process Communication&lt;/h1&gt;
&lt;p&gt;Note a task queue, threading, and MP all require a way for threads and processes to communicate with each other. Threads on a single machine can more easily share memory which makes them more efficient than MP. For Python threads, I typically use the standard Python &lt;a href="https://docs.python.org/3/library/queue.html"&gt;Queue lib&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Multiprocessing requires some form of &lt;a href="https://en.wikipedia.org/wiki/Inter-process_communication"&gt;inter-process communication or IPC&lt;/a&gt;, which attempts to serialize and deserialize objects between processes. Python provides &lt;a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process#pipes-and-queues"&gt;MP Queues and Pipes&lt;/a&gt; to &lt;a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process#exchanging-objects-between-processes"&gt;exchange objects between processes&lt;/a&gt;. I prefer the MP Queue, because it nearly mimics the API of the standard Python Queue lib, but because it is IPC, I have to be careful to only pass in objects that can be easily serialized and deserialized like numbers, strings, sequences, and dictionaries.&lt;/p&gt;
&lt;p&gt;For distributed computing, the recommendation is to use a message queue like &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; or a shared memory server like &lt;a href="https://memcached.org/"&gt;memcached&lt;/a&gt; or &lt;a href="https://redis.io/"&gt;reddis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, I don't have the expertise to say when to use an MQ verses memcache, but there are lots of articles on the internet discussing the pros/cons of each. I've used memcache and it works, but I believe for Celery they preffer MQ.&lt;/p&gt;
&lt;h2 id="queues"&gt;Queues&lt;/h2&gt;
&lt;p&gt;For both threads and processes, IMHO it's a good idea to keep the communication simple. I recommend only pushing the primitive types that will be needed by the target function handled by the thread and then reconstructing more complicated objects in the main branch of execution as needed. I believe this will keep the communication overhead low as well as make it easier to serialize and deserialize target arguments and avoid race conditions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: Python also uses the GIL to avoid race conditions between threads. See below.&lt;/p&gt;
&lt;h3 id="thread-safety-and-shared-memory"&gt;Thread Safety and Shared Memory&lt;/h3&gt;
&lt;p&gt;This is at the edge of my experience, but Python does provide tools to lock and synchronize communication between &lt;a href="https://docs.python.org/3/library/threading.html#lock-objects"&gt;threads&lt;/a&gt; and &lt;a href="https://docs.python.org/3/library/multiprocessing.html#synchronization-between-processes"&gt;processes&lt;/a&gt; as well as a &lt;a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process#sharing-state-between-processes"&gt;shared memory map for multiprocessing&lt;/a&gt;. Explore on your own.&lt;/p&gt;
&lt;h1 id="the-difference-between-io-and-cpu-bound-process"&gt;The Difference Between IO and CPU Bound Process&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An IO bound process is like getting data from a URL, or writing a file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A CPU bound process is like a big calculation like matrix inversion, a search routine, or optimization/minimzation problem&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IO bound processes use Threads because threads do not block IO bound processes - IE: you can start an IO process in a thread and execution will continue to the next line unless you tell Python to wait. AsyncIO and Trio probably use threading in the background, IDK for sure.&lt;/p&gt;
&lt;p&gt;CPU bound processes use MP, because they need their own core to do their stuff. But unfortunately there is a lot of overhead with MP, and so the calculation needs to be really expensive to make it worthwhile.&lt;/p&gt;
&lt;p&gt;There's no limit to the number of threads you can create, but the number of MP processes is limited to the number of cores.&lt;/p&gt;
&lt;p&gt;These rules don't apply to distributed computing, and they may not apply in other computing languages either. IE: you can distribute both IO and CPU tasks to a task queue across a cluster. However, there may be task managers that are better suited for different types of tasks. EG: &lt;a href="https://dask.org/"&gt;Dask&lt;/a&gt; is probably better for distributed computing of CPU intense processes. But honestly, IDK, I'm at the edge of my personal research and experience here, and so I would probably recommend a little more exploration ...&lt;/p&gt;
&lt;h1 id="python-global-interpreter-lock"&gt;Python Global Interpreter Lock&lt;/h1&gt;
&lt;p&gt;The reason there's this difference between IO/CPU and threads/processes in Python specifically, and perhaps not other languages, is because Python is single threaded by default due to the &lt;a href="https://wiki.python.org/moin/GlobalInterpreterLock"&gt;Global Interpreter Lock or GIL&lt;/a&gt;. Other languages may or may not have similar design, IDK, however Python C-API does let you explicitly &lt;a href="https://docs.python.org/3/c-api/init.html#thread-state-and-the-global-interpreter-lock"&gt;release the GIL&lt;/a&gt; and manage threads manually. This is what packages like &lt;a href="https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html"&gt;NumPy/SciPy&lt;/a&gt; and &lt;a href="https://cython.readthedocs.io/en/latest/src/userguide/parallelism.html"&gt;CPython&lt;/a&gt; do.&lt;/p&gt;</content><category term="Python"></category><category term="code"></category><category term="web"></category></entry><entry><title>Python-3 Virtual Environments on MS Windows</title><link href="https://breakingbytes.github.io/python-3-virtual-environments-on-ms-windows.html" rel="alternate"></link><published>2017-05-18T11:01:00-07:00</published><updated>2017-05-18T11:01:00-07:00</updated><author><name>Mark Mikofski</name></author><id>tag:breakingbytes.github.io,2017-05-18:/python-3-virtual-environments-on-ms-windows.html</id><summary type="html">&lt;p&gt;Python-3 venv module vs virtualenv package&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a follow up from my &lt;a href="https://breakingbytes.github.io/moving-to-pelican-at-github-pages.html"&gt;first post&lt;/a&gt;
on this new Pelican blog. I should have logged how I set up this, instead of
the glib cliché, "usage is a snap". Here I am posting for the second time, and
I'm wondering, "why didn't I just use the
&lt;a href="https://help.github.com/articles/about-github-pages-and-jekyll/"&gt;built in Jekyll static site generator&lt;/a&gt;?"&lt;/p&gt;
&lt;p&gt;Anyway, for posterity...&lt;/p&gt;
&lt;h1 id="publishing-to-gh-pages"&gt;Publishing to GH Pages&lt;/h1&gt;
&lt;p&gt;To publish my pages I use the &lt;code&gt;Makefile&lt;/code&gt; that Pelican generates when you start
your blog using &lt;code&gt;pelican-quickstart&lt;/code&gt;. I may have editted the &lt;code&gt;Makefile&lt;/code&gt; to
target my GitHub &lt;code&gt;master&lt;/code&gt; branch since I'm using
&lt;a href="http://docs.getpelican.com/en/stable/tips.html#user-pages"&gt;User Pages&lt;/a&gt; or maybe
Pelican asked me during the quickstart questionnaire, I can't remember. The
&lt;code&gt;Makefile&lt;/code&gt; uses a handy tool called
&lt;a href="https://github.com/davisp/ghp-import"&gt;&lt;code&gt;ghp-import&lt;/code&gt;&lt;/a&gt; that copies the contents of
the output directory to the target git branch. Then the &lt;code&gt;Makefile&lt;/code&gt; publishes
the blog by pushing the target branch to GitHub. The &lt;code&gt;ghp-import&lt;/code&gt;
package is described a bit in the
&lt;a href="http://docs.getpelican.com/en/stable/tips.html#publishing-to-github"&gt;Pelican Tips&lt;/a&gt;
on publishing to GitHub pages, but the &lt;code&gt;Makefile&lt;/code&gt; takes care of calling it. So
all I have to do to publish my posts is execute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make github
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You could even stick it into &lt;code&gt;.git/hooks/post-commit&lt;/code&gt; to automate publishing.&lt;/p&gt;
&lt;h1 id="python-3-virtual-environment"&gt;Python-3 Virtual Environment&lt;/h1&gt;
&lt;p&gt;Okay, I can't remeber exactly the reason, but &lt;code&gt;ghp-import&lt;/code&gt; worked better in
Python-3 than Python-2, and so I created a virtual environment for Python-3.
This brings me finally to the title of this post. How does one create a virtual
environment for Python-3? Well according to the
&lt;a href="https://packaging.python.org/installing/#creating-virtual-environments"&gt;Python Packaging Authority (PyPA)&lt;/a&gt;
since Python-3.3 there is a built in module called
&lt;a href="https://docs.python.org/3.5/library/venv.html"&gt;&lt;code&gt;venv&lt;/code&gt;&lt;/a&gt;. However the venerable
&lt;a href="https://virtualenv.pypa.io/en/stable/"&gt;&lt;code&gt;virtualenv&lt;/code&gt;&lt;/a&gt; package also works just
fine for Python-3.&lt;/p&gt;
&lt;p&gt;How do these packages differ? There is one major difference that really affects
me. The built in Python-3 &lt;code&gt;venv&lt;/code&gt; module only activates in MS Windows &lt;code&gt;CMD&lt;/code&gt;
terminal or PowerShell whereas Ian Bicking's indispensible &lt;code&gt;virtualenv&lt;/code&gt; package
works in BaSH as well. This is an issue with Python on MS Windows that I
encounter a lot, it's a PITA and breaks the whole concept of a common unified
experience regardless of user's platform. I should be able to use Python exactly
the same on any system with very minor exceptions. Since I tend to mostly
use BaSH, this means that to use the built in &lt;code&gt;venv&lt;/code&gt; module I have to
switch to a MS Windows &lt;code&gt;CMD&lt;/code&gt; shell. I guess it's not that big of a deal, but
since &lt;code&gt;virtualenv&lt;/code&gt; works fine with Python-3, I guess I'll stick with that.&lt;/p&gt;
&lt;p&gt;Okay, there's also one other important difference. Python-3 installs all of its
shared objects into the virtual environment, which is different from Python-2
which uses links mostly. This means that when you update your version of
Python-3 you need to &lt;em&gt;also&lt;/em&gt; update your virtual environment.&lt;/p&gt;
&lt;p&gt;For the &lt;code&gt;virtualenv&lt;/code&gt; package you'll have to create a new virtual environment on
top of the old one. But for the built in &lt;code&gt;venv&lt;/code&gt; module you can just run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;py -3 -m venv --upgrade &amp;lt;my-py3-venv&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code&gt;&amp;lt;my-py3-venv&amp;gt;&lt;/code&gt; is the name of your Python-3 virtual environment. I think
the source code for both are nearly the same, and I also think that under the
hood the &lt;code&gt;--upgrade&lt;/code&gt; option is really doing exactly the same thing as
&lt;code&gt;virtualenv&lt;/code&gt;, the difference is that if you try to create a virtual environment
on top of an existing one with the Python-3 &lt;code&gt;venv&lt;/code&gt; module is will raise an
exception.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Changed in version 3.4: In earlier versions, if the target directory already
existed, an error was raised, unless the &lt;code&gt;--clear&lt;/code&gt; or &lt;code&gt;--upgrade&lt;/code&gt; option was
provided. Now, if an existing directory is specified, its contents are removed
and the directory is processed as if it had been newly created.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In conclusion, I don't see much difference between the two methods. So if you
want to use BaSH on MS Windows, stick with the original, otherwise try the new.&lt;/p&gt;</content><category term="Python"></category><category term="virtualenv"></category><category term="pelican"></category><category term="windows"></category><category term="bash"></category><category term="rant"></category></entry></feed>